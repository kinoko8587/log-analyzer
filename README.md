# ğŸ“Š Log Analyzer 

A high-concurrency log ingestion and analytics system built in Go.  
This project simulates real-world backend challenges like message queue ingestion, batch aggregation, caching, and system observability â€” all built with **Clean Architecture** and **modular services**.


## âœ¨ Features

- Modular service design (API, Ingestor, Batch Job)
- Kafka-based log ingestion (async + scalable)
- PostgreSQL log storage & denormalized summary tables
- RESTful APIs with Gin + Redis caching
- Batch job for daily aggregation & keyword stats
- Prometheus metrics / pprof profiling / graceful shutdown
- CI/CD with GitHub Actions & containerized environment

---

## ğŸ—‚ Project Structure

```bash
log-analyzer/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ api-server/        # REST API server (dashboard & stats)
â”‚   â”œâ”€â”€ log-ingestor/      # Kafka consumer to persist logs to DB
â”‚   â””â”€â”€ batch-job/         # Daily aggregation job
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ domain/            # Entity models & interface definitions
â”‚   â”œâ”€â”€ usecase/           # Application logic (analyze, query, etc.)
â”‚   â”œâ”€â”€ infrastructure/    # DB, Redis, Kafka adapters
â”‚   â”œâ”€â”€ interface/         
â”‚   â”‚   â”œâ”€â”€ http/          # REST handlers
â”‚   â”‚   â””â”€â”€ grpc/          # (Optional) gRPC handlers
â”‚   â””â”€â”€ scheduler/         # cron-like batch executor
â”œâ”€â”€ pkg/                   # Shared utilities and log models
â”œâ”€â”€ migrations/            # DB migration SQL
â”œâ”€â”€ config/                # YAML/env configuration
â”œâ”€â”€ deploy/                # Docker Compose, CI/CD files
â””â”€â”€ README.md
```

ğŸ“¦ Services Overview
| Service	|Description|
|----|-----|
|api-server|	Exposes REST APIs for log stats/dashboard|
|log-ingestor|	Listens to Kafka and writes logs to DB|
|batch-job	|Aggregates daily user stats and keywords|

## ğŸ§© Phase Roadmap
### âœ… Phase 1: MVP Setup - Kafka Ingestion
Focus: replacing in-memory storage with Kafka â†’ PostgreSQL

- [x] Set up Kafka + Zookeeper via Docker Compose

- [x] Create Kafka Producer (mock log generator)

- [ ] Create Kafka Consumer service (log-ingestor)

- [ ] Design PostgreSQL schema log_raw (normalized)

- [ ] Implement LogRepository interface

- [ ] Abstract Kafka via QueueSubscriber interface

- [ ] Add graceful shutdown & error handling for workers

### ğŸš€ Phase 2: API Server + Redis Cache
Focus: API endpoints + caching for dashboard queries

- [ ] Create REST API server using Gin (api-server)

- [ ] Define routes:

    - /stats/errors?window=10s

    - /stats/keywords?q=timeout

- [ ] Add Redis cache layer for recent queries

- [ ] Implement fallback: Redis miss â†’ DB â†’ re-cache

- [ ] Add JWT-based auth (optional)

### ğŸ“Š Phase 3: Batch Job for Aggregation
Focus: summary tables and offline aggregation logic

- [ ] Create batch-job binary

- [ ] Cron schedule aggregation every 1 day

- [ ] Create table user_log_summary (denormalized)

- [ ] Aggregate: total logs, error rate, keyword frequency

- [ ] Insert into summary table

- [ ] Track & log invalid/malformed rows

### â˜ï¸ Phase 4: Observability & Metrics
Focus: system introspection & monitoring

- [ ] Add Prometheus metrics (/metrics)

    - log ingestion rate

    - error count per worker

    - batch job duration

- [ ] Enable pprof (/debug/pprof)

- [ ] Add structured logging (zap or zerolog)

- [ ] Log collector ready (stdout or file)

### ğŸ” Phase 5: CI/CD + Deploy
Focus: DevOps and reproducible environments

- [ ] Create docker-compose.yml with:
    - Kafka + Zookeeper
    - PostgreSQL
    - Redis
    - All 3 services

- [ ] Add .env and YAML config loaders

- [ ] Create GitHub Actions workflow:
    - test
    - lint
    - docker build
- [ ] Add Makefile for build/dev tasks

ğŸ›  Tools Used
|Area	    |Tech|
|-----------|-----|
|Language	|Go 1.21+|
|Queue	    |Kafka (segmentio/kafka-go)|
|DB	        |PostgreSQL / SQLite|
|API        |	Gin|
|Cache      |	Redis (go-redis)|
|Metrics	|Prometheus client_golang|
|Profiling	|net/http/pprof|
|Testing	|Go test, stretchr/testify|
|Deployment	|Docker, GitHub Actions|

ğŸ“Œ How to Run (MVP Phase)
```
bash
# Start Kafka + PostgreSQL + services
docker-compose up -d

# Run log producer to push logs into Kafka
go run cmd/log-generator/main.go

# Run Kafka consumer to persist to DB
go run cmd/log-ingestor/main.go

# Start API server
go run cmd/api-server/main.go

# Check stats
curl http://localhost:8080/stats/errors
```
## ğŸ§  Key Concepts Practiced
- Clean Architecture in Go
- Message queue (Kafka) consumer pipelines
- High-concurrency goroutine orchestration
- Time-windowed stat computation
- Caching & fallback strategies (Redis)
- System observability with Prometheus
- Modular service decomposition

## ğŸ§ª Future Ideas
- gRPC endpoints with protobuf
- SQLite fallback mode (embedded DB)
- Live dashboard with WebSocket streaming
- ElasticSearch log search integration

### ğŸ§‘â€ğŸ’» Author
Ann Chen